# ðŸš€ Simple LLM Application using LCEL

This project demonstrates a **minimal and clean implementation of a Large Language Model (LLM)** using **LangChain Expression Language (LCEL)**. It focuses on building readable, composable, and maintainable LLM pipelines without introducing complex agent orchestration or tool-based reasoning.

The application uses LCEL to define the flow of data between prompts, models, and output parsers in a declarative manner. This approach makes the logic easy to understand, debug, and extend while keeping the codebase lightweight.

Key characteristics of this implementation include:
- A **single-pass LLM workflow** using LCEL primitives
- Clear separation between **prompt definition**, **model invocation**, and **response handling**
- Easy adaptability for future extensions such as tools, memory, or graph-based agents

This project is ideal for learning LCEL fundamentals, prototyping LLM-powered features quickly, or serving as a foundation before evolving into more advanced LangGraph or agent-based architectures.

